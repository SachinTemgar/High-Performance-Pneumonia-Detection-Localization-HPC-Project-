{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6951a9f7-7506-43fa-9ff0-ad3d9df9a4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ddp_train_cpu.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch.multiprocessing as mp\n",
    "import time\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class PneumoniaDataset(Dataset):\n",
    "    def __init__(self, Image_dir, Mask_dir, transform=None):\n",
    "        self.image_paths = sorted([os.path.join(Image_dir, f) for f in os.listdir(Image_dir)\n",
    "                                   if f.endswith(('.jpeg', '.jpg', '.png'))])\n",
    "        self.mask_paths = sorted([os.path.join(Mask_dir, f) for f in os.listdir(Mask_dir)\n",
    "                                  if f.endswith(('.jpeg', '.jpg', '.png'))])\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.image_paths[idx]).convert('L')\n",
    "        mask = Image.open(self.mask_paths[idx]).convert('L')\n",
    "\n",
    "        label = 1 if mask.getextrema()[1] > 0 else 0  \n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "\n",
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "\n",
    "        self._to_linear = None\n",
    "        self._get_flattened_size()\n",
    "        self.fc1 = nn.Linear(self._to_linear, 128)\n",
    "        self.fc2 = nn.Linear(128, 2)\n",
    "\n",
    "    def _get_flattened_size(self):\n",
    "        with torch.no_grad():\n",
    "            x = torch.zeros(1, 1, 256, 256)\n",
    "            x = self.pool(F.relu(self.conv1(x)))\n",
    "            x = self.pool(F.relu(self.conv2(x)))\n",
    "            x = self.dropout1(x)\n",
    "            self._to_linear = x.view(1, -1).shape[1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def train_model(rank, world_size, args):\n",
    "    os.environ[\"MASTER_ADDR\"] = \"127.0.0.1\"\n",
    "    os.environ[\"MASTER_PORT\"] = \"29500\"\n",
    "\n",
    "    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    dataset = PneumoniaDataset(args.image_dir, args.mask_dir, transform)\n",
    "\n",
    "    sampler = torch.utils.data.distributed.DistributedSampler(dataset, num_replicas=world_size, rank=rank)\n",
    "    dataloader = DataLoader(dataset, batch_size=32, sampler=sampler, num_workers=4, pin_memory=True)\n",
    "\n",
    "    device = torch.device(f\"cpu\")  \n",
    "    model = CNNClassifier().to(device)\n",
    "    model = DDP(model, device_ids=None)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    \n",
    "    epoch_train_losses = []\n",
    "    epoch_accuracies = []\n",
    "\n",
    "    for epoch in range(1, 6):\n",
    "        model.train()\n",
    "        sampler.set_epoch(epoch)\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        start = time.time()\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(dataloader):\n",
    "            data = data.to(device)\n",
    "            target = torch.tensor(target).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            correct += (predicted == target).sum().item()\n",
    "            total += target.size(0)\n",
    "            \n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"[Rank {rank}] Epoch {epoch} | Batch {batch_idx} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        epoch_train_losses.append(running_loss / len(dataloader))\n",
    "        epoch_accuracies.append(accuracy)\n",
    "\n",
    "        end = time.time()\n",
    "        print(f\"⏱️ [Rank {rank}] Epoch {epoch} completed in {end - start:.2f} seconds\")\n",
    "        print(f\"⏳ [Rank {rank}] Epoch {epoch} | Training Loss: {epoch_train_losses[-1]:.4f} | Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "\n",
    "    if rank == 0:\n",
    "        torch.save(model.state_dict(), \"cnn_ddp_model.pth\")\n",
    "        print(\"✅ Model saved as cnn_ddp_model.pth\", flush=True)\n",
    "\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "    if rank == 0:\n",
    "        plot_graphs(epoch_train_losses, epoch_accuracies)\n",
    "\n",
    "\n",
    "def plot_graphs(losses, accuracies):\n",
    "    epochs = range(1, len(losses) + 1)\n",
    "\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, losses, label='Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss over Epochs')\n",
    "    plt.legend()\n",
    "\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, accuracies, label='Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Accuracy over Epochs')\n",
    "    plt.legend()\n",
    "\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_loss_accuracy_plots.png')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--image-dir', type=str, required=True, help='Path to training images')\n",
    "    parser.add_argument('--mask-dir', type=str, required=True, help='Path to training masks')\n",
    "    parser.add_argument('--save-dir', type=str, default=\"/scratch/mohammed.moi/yolo_workspace\", help='Where to save model')\n",
    "    parser.add_argument('--world-size', type=int, default=4, help='Number of parallel processes')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    world_size = args.world_size\n",
    "    mp.spawn(train_model, args=(world_size, args), nprocs=world_size, join=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c756edf7-1a3e-4da2-bed0-e447f6a2e760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mohammed.moi/ddp_train_cpu.py:109: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target = torch.tensor(target).to(device)\n",
      "/home/mohammed.moi/ddp_train_cpu.py:109: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target = torch.tensor(target).to(device)\n",
      "/home/mohammed.moi/ddp_train_cpu.py:109: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target = torch.tensor(target).to(device)\n",
      "/home/mohammed.moi/ddp_train_cpu.py:109: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target = torch.tensor(target).to(device)\n",
      "[Rank 3] Epoch 1 | Batch 0 | Loss: 0.7236\n",
      "[Rank 2] Epoch 1 | Batch 0 | Loss: 0.7149\n",
      "[Rank 1] Epoch 1 | Batch 0 | Loss: 0.7297\n",
      "[Rank 0] Epoch 1 | Batch 0 | Loss: 0.7302\n",
      "[Rank 3] Epoch 1 | Batch 10 | Loss: 0.7267\n",
      "[Rank 0] Epoch 1 | Batch 10 | Loss: 0.7136\n",
      "[Rank 2] Epoch 1 | Batch 10 | Loss: 0.7308\n",
      "[Rank 1] Epoch 1 | Batch 10 | Loss: 0.7185\n",
      "[Rank 0] Epoch 1 | Batch 20 | Loss: 0.5279\n",
      "[Rank 1] Epoch 1 | Batch 20 | Loss: 0.5257\n",
      "[Rank 2] Epoch 1 | Batch 20 | Loss: 0.5599\n",
      "[Rank 3] Epoch 1 | Batch 20 | Loss: 0.4490\n",
      "[Rank 1] Epoch 1 | Batch 30 | Loss: 0.5314\n",
      "[Rank 2] Epoch 1 | Batch 30 | Loss: 0.5509\n",
      "[Rank 3] Epoch 1 | Batch 30 | Loss: 0.4579\n",
      "[Rank 0] Epoch 1 | Batch 30 | Loss: 0.5079\n",
      "[Rank 2] Epoch 1 | Batch 40 | Loss: 0.5575\n",
      "[Rank 3] Epoch 1 | Batch 40 | Loss: 0.5562\n",
      "[Rank 0] Epoch 1 | Batch 40 | Loss: 0.5383\n",
      "[Rank 1] Epoch 1 | Batch 40 | Loss: 0.5138\n",
      "[Rank 1] Epoch 1 | Batch 50 | Loss: 0.5887\n",
      "[Rank 0] Epoch 1 | Batch 50 | Loss: 0.4194\n",
      "[Rank 3] Epoch 1 | Batch 50 | Loss: 0.6395\n",
      "[Rank 2] Epoch 1 | Batch 50 | Loss: 0.5186\n",
      "[Rank 2] Epoch 1 | Batch 60 | Loss: 0.4151\n",
      "[Rank 3] Epoch 1 | Batch 60 | Loss: 0.5329\n",
      "[Rank 0] Epoch 1 | Batch 60 | Loss: 0.4587\n",
      "[Rank 1] Epoch 1 | Batch 60 | Loss: 0.5900\n",
      "[Rank 0] Epoch 1 | Batch 70 | Loss: 0.2913\n",
      "[Rank 3] Epoch 1 | Batch 70 | Loss: 0.3855\n",
      "[Rank 2] Epoch 1 | Batch 70 | Loss: 0.4027\n",
      "[Rank 1] Epoch 1 | Batch 70 | Loss: 0.4416\n",
      "[Rank 3] Epoch 1 | Batch 80 | Loss: 0.4057\n",
      "[Rank 2] Epoch 1 | Batch 80 | Loss: 0.3462\n",
      "[Rank 1] Epoch 1 | Batch 80 | Loss: 0.4201\n",
      "[Rank 0] Epoch 1 | Batch 80 | Loss: 0.4235\n",
      "[Rank 1] Epoch 1 | Batch 90 | Loss: 0.3050\n",
      "[Rank 0] Epoch 1 | Batch 90 | Loss: 0.7383\n",
      "[Rank 2] Epoch 1 | Batch 90 | Loss: 0.4531\n",
      "[Rank 3] Epoch 1 | Batch 90 | Loss: 0.4416\n",
      "[Rank 0] Epoch 1 | Batch 100 | Loss: 0.4772\n",
      "[Rank 1] Epoch 1 | Batch 100 | Loss: 0.5382\n",
      "[Rank 3] Epoch 1 | Batch 100 | Loss: 0.5416\n",
      "[Rank 2] Epoch 1 | Batch 100 | Loss: 0.4177\n",
      "[Rank 2] Epoch 1 | Batch 110 | Loss: 0.4707\n",
      "[Rank 1] Epoch 1 | Batch 110 | Loss: 0.2495\n",
      "[Rank 0] Epoch 1 | Batch 110 | Loss: 0.3742\n",
      "[Rank 3] Epoch 1 | Batch 110 | Loss: 0.3907\n",
      "[Rank 3] Epoch 1 | Batch 120 | Loss: 0.3727\n",
      "[Rank 0] Epoch 1 | Batch 120 | Loss: 0.4173\n",
      "[Rank 2] Epoch 1 | Batch 120 | Loss: 0.4926\n",
      "[Rank 1] Epoch 1 | Batch 120 | Loss: 0.6697\n",
      "[Rank 2] Epoch 1 | Batch 130 | Loss: 0.3586\n",
      "[Rank 0] Epoch 1 | Batch 130 | Loss: 0.5728\n",
      "[Rank 1] Epoch 1 | Batch 130 | Loss: 0.4478\n",
      "[Rank 3] Epoch 1 | Batch 130 | Loss: 0.6099\n",
      "[Rank 3] Epoch 1 | Batch 140 | Loss: 0.2677\n",
      "[Rank 0] Epoch 1 | Batch 140 | Loss: 0.3545\n",
      "[Rank 2] Epoch 1 | Batch 140 | Loss: 0.2867\n",
      "[Rank 1] Epoch 1 | Batch 140 | Loss: 0.3932\n",
      "[Rank 0] Epoch 1 | Batch 150 | Loss: 0.5106\n",
      "[Rank 2] Epoch 1 | Batch 150 | Loss: 0.3449\n",
      "[Rank 3] Epoch 1 | Batch 150 | Loss: 0.5232\n",
      "[Rank 1] Epoch 1 | Batch 150 | Loss: 0.4287\n",
      "[Rank 2] Epoch 1 | Batch 160 | Loss: 0.4616\n",
      "[Rank 1] Epoch 1 | Batch 160 | Loss: 0.3283\n",
      "[Rank 3] Epoch 1 | Batch 160 | Loss: 0.4687\n",
      "[Rank 0] Epoch 1 | Batch 160 | Loss: 0.4552\n",
      "[Rank 3] Epoch 1 | Batch 170 | Loss: 0.5663\n",
      "[Rank 2] Epoch 1 | Batch 170 | Loss: 0.4043\n",
      "[Rank 1] Epoch 1 | Batch 170 | Loss: 0.4290\n",
      "[Rank 0] Epoch 1 | Batch 170 | Loss: 0.4982\n",
      "[Rank 3] Epoch 1 | Batch 180 | Loss: 0.6810\n",
      "[Rank 2] Epoch 1 | Batch 180 | Loss: 0.3234\n",
      "[Rank 0] Epoch 1 | Batch 180 | Loss: 0.2702\n",
      "[Rank 1] Epoch 1 | Batch 180 | Loss: 0.3591\n",
      "[Rank 0] Epoch 1 | Batch 190 | Loss: 0.6510\n",
      "[Rank 2] Epoch 1 | Batch 190 | Loss: 0.3706\n",
      "[Rank 1] Epoch 1 | Batch 190 | Loss: 0.4606\n",
      "[Rank 3] Epoch 1 | Batch 190 | Loss: 0.5144\n",
      "[Rank 3] Epoch 1 | Batch 200 | Loss: 0.3245\n",
      "[Rank 1] Epoch 1 | Batch 200 | Loss: 0.4052\n",
      "[Rank 2] Epoch 1 | Batch 200 | Loss: 0.5261\n",
      "[Rank 0] Epoch 1 | Batch 200 | Loss: 0.5083\n",
      "⏱️ [Rank 2] Epoch 1 completed in 473.22 seconds\n",
      "⏱️ [Rank 1] Epoch 1 completed in 473.19 seconds\n",
      "⏱️ [Rank 0] Epoch 1 completed in 473.30 seconds\n",
      "⏱️ [Rank 3] Epoch 1 completed in 473.36 seconds\n",
      "[Rank 2] Epoch 2 | Batch 0 | Loss: 0.3176\n",
      "[Rank 3] Epoch 2 | Batch 0 | Loss: 0.5826\n",
      "[Rank 0] Epoch 2 | Batch 0 | Loss: 0.4109\n",
      "[Rank 1] Epoch 2 | Batch 0 | Loss: 0.5784\n",
      "[Rank 1] Epoch 2 | Batch 10 | Loss: 0.6232\n",
      "[Rank 3] Epoch 2 | Batch 10 | Loss: 0.4156\n",
      "[Rank 2] Epoch 2 | Batch 10 | Loss: 0.2766\n",
      "[Rank 0] Epoch 2 | Batch 10 | Loss: 0.4069\n",
      "[Rank 0] Epoch 2 | Batch 20 | Loss: 0.4152\n",
      "[Rank 2] Epoch 2 | Batch 20 | Loss: 0.4331\n",
      "[Rank 3] Epoch 2 | Batch 20 | Loss: 0.3942\n",
      "[Rank 1] Epoch 2 | Batch 20 | Loss: 0.4172\n",
      "[Rank 3] Epoch 2 | Batch 30 | Loss: 0.5472\n",
      "[Rank 0] Epoch 2 | Batch 30 | Loss: 0.6342\n",
      "[Rank 1] Epoch 2 | Batch 30 | Loss: 0.4279\n",
      "[Rank 2] Epoch 2 | Batch 30 | Loss: 0.6475\n",
      "[Rank 0] Epoch 2 | Batch 40 | Loss: 0.3650\n",
      "[Rank 3] Epoch 2 | Batch 40 | Loss: 0.4924\n",
      "[Rank 1] Epoch 2 | Batch 40 | Loss: 0.3675\n",
      "[Rank 2] Epoch 2 | Batch 40 | Loss: 0.4312\n",
      "[Rank 2] Epoch 2 | Batch 50 | Loss: 0.2738\n",
      "[Rank 0] Epoch 2 | Batch 50 | Loss: 0.3508\n",
      "[Rank 3] Epoch 2 | Batch 50 | Loss: 0.4422\n",
      "[Rank 1] Epoch 2 | Batch 50 | Loss: 0.2680\n",
      "[Rank 2] Epoch 2 | Batch 60 | Loss: 0.4379\n",
      "[Rank 3] Epoch 2 | Batch 60 | Loss: 0.4183\n",
      "[Rank 0] Epoch 2 | Batch 60 | Loss: 0.5210\n",
      "[Rank 1] Epoch 2 | Batch 60 | Loss: 0.4606\n",
      "[Rank 3] Epoch 2 | Batch 70 | Loss: 0.3877\n",
      "[Rank 0] Epoch 2 | Batch 70 | Loss: 0.3350\n",
      "[Rank 1] Epoch 2 | Batch 70 | Loss: 0.4187\n",
      "[Rank 2] Epoch 2 | Batch 70 | Loss: 0.6419\n",
      "[Rank 1] Epoch 2 | Batch 80 | Loss: 0.4793\n",
      "[Rank 2] Epoch 2 | Batch 80 | Loss: 0.5825\n",
      "[Rank 0] Epoch 2 | Batch 80 | Loss: 0.6061\n",
      "[Rank 3] Epoch 2 | Batch 80 | Loss: 0.3110\n",
      "[Rank 2] Epoch 2 | Batch 90 | Loss: 0.2927\n",
      "[Rank 1] Epoch 2 | Batch 90 | Loss: 0.4714\n",
      "[Rank 3] Epoch 2 | Batch 90 | Loss: 0.5235\n",
      "[Rank 0] Epoch 2 | Batch 90 | Loss: 0.2620\n",
      "[Rank 3] Epoch 2 | Batch 100 | Loss: 0.3521\n",
      "[Rank 0] Epoch 2 | Batch 100 | Loss: 0.6154\n",
      "[Rank 1] Epoch 2 | Batch 100 | Loss: 0.4045\n",
      "[Rank 2] Epoch 2 | Batch 100 | Loss: 0.5291\n",
      "[Rank 0] Epoch 2 | Batch 110 | Loss: 0.4570\n",
      "[Rank 1] Epoch 2 | Batch 110 | Loss: 0.5208\n",
      "[Rank 3] Epoch 2 | Batch 110 | Loss: 0.4221\n",
      "[Rank 2] Epoch 2 | Batch 110 | Loss: 0.4676\n",
      "[Rank 0] Epoch 2 | Batch 120 | Loss: 0.4470\n",
      "[Rank 2] Epoch 2 | Batch 120 | Loss: 0.4080\n",
      "[Rank 1] Epoch 2 | Batch 120 | Loss: 0.4157\n",
      "[Rank 3] Epoch 2 | Batch 120 | Loss: 0.4295\n",
      "[Rank 0] Epoch 2 | Batch 130 | Loss: 0.4173\n",
      "[Rank 1] Epoch 2 | Batch 130 | Loss: 0.3422\n",
      "[Rank 2] Epoch 2 | Batch 130 | Loss: 0.2566\n",
      "[Rank 3] Epoch 2 | Batch 130 | Loss: 0.5460\n",
      "[Rank 2] Epoch 2 | Batch 140 | Loss: 0.4850\n",
      "[Rank 3] Epoch 2 | Batch 140 | Loss: 0.4642\n",
      "[Rank 1] Epoch 2 | Batch 140 | Loss: 0.3950\n",
      "[Rank 0] Epoch 2 | Batch 140 | Loss: 0.6023\n",
      "[Rank 1] Epoch 2 | Batch 150 | Loss: 0.4108\n",
      "[Rank 3] Epoch 2 | Batch 150 | Loss: 0.3455\n",
      "[Rank 0] Epoch 2 | Batch 150 | Loss: 0.5677\n",
      "[Rank 2] Epoch 2 | Batch 150 | Loss: 0.4128\n",
      "[Rank 1] Epoch 2 | Batch 160 | Loss: 0.3890\n",
      "[Rank 0] Epoch 2 | Batch 160 | Loss: 0.5463\n",
      "[Rank 3] Epoch 2 | Batch 160 | Loss: 0.5372\n",
      "[Rank 2] Epoch 2 | Batch 160 | Loss: 0.3074\n",
      "[Rank 1] Epoch 2 | Batch 170 | Loss: 0.2427\n",
      "[Rank 3] Epoch 2 | Batch 170 | Loss: 0.5650\n",
      "[Rank 0] Epoch 2 | Batch 170 | Loss: 0.4054\n",
      "[Rank 2] Epoch 2 | Batch 170 | Loss: 0.5833\n",
      "[Rank 1] Epoch 2 | Batch 180 | Loss: 0.3638\n",
      "[Rank 2] Epoch 2 | Batch 180 | Loss: 0.3009\n",
      "[Rank 3] Epoch 2 | Batch 180 | Loss: 0.3992\n",
      "[Rank 0] Epoch 2 | Batch 180 | Loss: 0.4482\n",
      "[Rank 2] Epoch 2 | Batch 190 | Loss: 0.3107\n",
      "[Rank 3] Epoch 2 | Batch 190 | Loss: 0.4378\n",
      "[Rank 0] Epoch 2 | Batch 190 | Loss: 0.5604\n",
      "[Rank 1] Epoch 2 | Batch 190 | Loss: 0.4119\n",
      "[Rank 3] Epoch 2 | Batch 200 | Loss: 0.5969\n",
      "[Rank 2] Epoch 2 | Batch 200 | Loss: 0.3166\n",
      "[Rank 0] Epoch 2 | Batch 200 | Loss: 0.4752\n",
      "[Rank 1] Epoch 2 | Batch 200 | Loss: 0.6070\n",
      "⏱️ [Rank 1] Epoch 2 completed in 454.51 seconds\n",
      "⏱️ [Rank 0] Epoch 2 completed in 454.52 seconds\n",
      "⏱️ [Rank 3] Epoch 2 completed in 454.43 seconds\n",
      "⏱️ [Rank 2] Epoch 2 completed in 454.62 seconds\n",
      "[Rank 1] Epoch 3 | Batch 0 | Loss: 0.2281\n",
      "[Rank 2] Epoch 3 | Batch 0 | Loss: 0.4845\n",
      "[Rank 3] Epoch 3 | Batch 0 | Loss: 0.4637\n",
      "[Rank 0] Epoch 3 | Batch 0 | Loss: 0.3646\n",
      "[Rank 1] Epoch 3 | Batch 10 | Loss: 0.4883\n",
      "[Rank 0] Epoch 3 | Batch 10 | Loss: 0.4070\n",
      "[Rank 3] Epoch 3 | Batch 10 | Loss: 0.3616\n",
      "[Rank 2] Epoch 3 | Batch 10 | Loss: 0.3975\n",
      "[Rank 0] Epoch 3 | Batch 20 | Loss: 0.5263\n",
      "[Rank 1] Epoch 3 | Batch 20 | Loss: 0.4871\n",
      "[Rank 3] Epoch 3 | Batch 20 | Loss: 0.5400\n",
      "[Rank 2] Epoch 3 | Batch 20 | Loss: 0.6604\n",
      "[Rank 1] Epoch 3 | Batch 30 | Loss: 0.5443\n",
      "[Rank 2] Epoch 3 | Batch 30 | Loss: 0.2621\n",
      "[Rank 0] Epoch 3 | Batch 30 | Loss: 0.4796\n",
      "[Rank 3] Epoch 3 | Batch 30 | Loss: 0.3335\n",
      "[Rank 0] Epoch 3 | Batch 40 | Loss: 0.4994\n",
      "[Rank 2] Epoch 3 | Batch 40 | Loss: 0.5572\n",
      "[Rank 3] Epoch 3 | Batch 40 | Loss: 0.4343\n",
      "[Rank 1] Epoch 3 | Batch 40 | Loss: 0.5501\n",
      "[Rank 2] Epoch 3 | Batch 50 | Loss: 0.3376\n",
      "[Rank 1] Epoch 3 | Batch 50 | Loss: 0.3663\n",
      "[Rank 0] Epoch 3 | Batch 50 | Loss: 0.4932\n",
      "[Rank 3] Epoch 3 | Batch 50 | Loss: 0.4223\n",
      "[Rank 2] Epoch 3 | Batch 60 | Loss: 0.5692\n",
      "[Rank 3] Epoch 3 | Batch 60 | Loss: 0.5916\n",
      "[Rank 1] Epoch 3 | Batch 60 | Loss: 0.4862\n",
      "[Rank 0] Epoch 3 | Batch 60 | Loss: 0.3945\n",
      "[Rank 1] Epoch 3 | Batch 70 | Loss: 0.5525\n",
      "[Rank 0] Epoch 3 | Batch 70 | Loss: 0.4906\n",
      "[Rank 3] Epoch 3 | Batch 70 | Loss: 0.4134\n",
      "[Rank 2] Epoch 3 | Batch 70 | Loss: 0.4678\n",
      "[Rank 3] Epoch 3 | Batch 80 | Loss: 0.3000\n",
      "[Rank 1] Epoch 3 | Batch 80 | Loss: 0.5592\n",
      "[Rank 2] Epoch 3 | Batch 80 | Loss: 0.4317\n",
      "[Rank 0] Epoch 3 | Batch 80 | Loss: 0.3691\n",
      "[Rank 1] Epoch 3 | Batch 90 | Loss: 0.3577\n",
      "[Rank 2] Epoch 3 | Batch 90 | Loss: 0.4141\n",
      "[Rank 3] Epoch 3 | Batch 90 | Loss: 0.4324\n",
      "[Rank 0] Epoch 3 | Batch 90 | Loss: 0.4645\n",
      "[Rank 1] Epoch 3 | Batch 100 | Loss: 0.4229\n",
      "[Rank 0] Epoch 3 | Batch 100 | Loss: 0.2890\n",
      "[Rank 3] Epoch 3 | Batch 100 | Loss: 0.5233\n",
      "[Rank 2] Epoch 3 | Batch 100 | Loss: 0.2921\n",
      "[Rank 3] Epoch 3 | Batch 110 | Loss: 0.3080\n",
      "[Rank 2] Epoch 3 | Batch 110 | Loss: 0.5363\n",
      "[Rank 1] Epoch 3 | Batch 110 | Loss: 0.3670\n",
      "[Rank 0] Epoch 3 | Batch 110 | Loss: 0.3665\n",
      "[Rank 1] Epoch 3 | Batch 120 | Loss: 0.4739\n",
      "[Rank 3] Epoch 3 | Batch 120 | Loss: 0.3345\n",
      "[Rank 0] Epoch 3 | Batch 120 | Loss: 0.4496\n",
      "[Rank 2] Epoch 3 | Batch 120 | Loss: 0.3228\n",
      "[Rank 1] Epoch 3 | Batch 130 | Loss: 0.5719\n",
      "[Rank 0] Epoch 3 | Batch 130 | Loss: 0.3494\n",
      "[Rank 3] Epoch 3 | Batch 130 | Loss: 0.4635\n",
      "[Rank 2] Epoch 3 | Batch 130 | Loss: 0.2901\n",
      "[Rank 1] Epoch 3 | Batch 140 | Loss: 0.4740\n",
      "[Rank 0] Epoch 3 | Batch 140 | Loss: 0.4352\n",
      "[Rank 3] Epoch 3 | Batch 140 | Loss: 0.4403\n",
      "[Rank 2] Epoch 3 | Batch 140 | Loss: 0.4769\n",
      "[Rank 0] Epoch 3 | Batch 150 | Loss: 0.4113\n",
      "[Rank 1] Epoch 3 | Batch 150 | Loss: 0.3687\n",
      "[Rank 3] Epoch 3 | Batch 150 | Loss: 0.5949\n",
      "[Rank 2] Epoch 3 | Batch 150 | Loss: 0.4195\n",
      "[Rank 1] Epoch 3 | Batch 160 | Loss: 0.5987\n",
      "[Rank 3] Epoch 3 | Batch 160 | Loss: 0.5933\n",
      "[Rank 0] Epoch 3 | Batch 160 | Loss: 0.3860\n",
      "[Rank 2] Epoch 3 | Batch 160 | Loss: 0.2686\n",
      "[Rank 3] Epoch 3 | Batch 170 | Loss: 0.4144\n",
      "[Rank 1] Epoch 3 | Batch 170 | Loss: 0.5516\n",
      "[Rank 0] Epoch 3 | Batch 170 | Loss: 0.4758\n",
      "[Rank 2] Epoch 3 | Batch 170 | Loss: 0.5019\n",
      "[Rank 0] Epoch 3 | Batch 180 | Loss: 0.4849\n",
      "[Rank 1] Epoch 3 | Batch 180 | Loss: 0.3488\n",
      "[Rank 2] Epoch 3 | Batch 180 | Loss: 0.4039\n",
      "[Rank 3] Epoch 3 | Batch 180 | Loss: 0.3819\n",
      "[Rank 2] Epoch 3 | Batch 190 | Loss: 0.4432\n",
      "[Rank 0] Epoch 3 | Batch 190 | Loss: 0.4289\n",
      "[Rank 1] Epoch 3 | Batch 190 | Loss: 0.5292\n",
      "[Rank 3] Epoch 3 | Batch 190 | Loss: 0.3348\n",
      "[Rank 1] Epoch 3 | Batch 200 | Loss: 0.4441\n",
      "[Rank 2] Epoch 3 | Batch 200 | Loss: 0.5614\n",
      "[Rank 3] Epoch 3 | Batch 200 | Loss: 0.4042\n",
      "[Rank 0] Epoch 3 | Batch 200 | Loss: 0.3512\n",
      "⏱️ [Rank 1] Epoch 3 completed in 471.01 seconds\n",
      "⏱️ [Rank 2] Epoch 3 completed in 471.01 seconds\n",
      "⏱️ [Rank 3] Epoch 3 completed in 471.09 seconds\n",
      "⏱️ [Rank 0] Epoch 3 completed in 471.10 seconds\n",
      "[Rank 1] Epoch 4 | Batch 0 | Loss: 0.4926\n",
      "[Rank 0] Epoch 4 | Batch 0 | Loss: 0.3917\n",
      "[Rank 2] Epoch 4 | Batch 0 | Loss: 0.3800\n",
      "[Rank 3] Epoch 4 | Batch 0 | Loss: 0.3993\n",
      "[Rank 1] Epoch 4 | Batch 10 | Loss: 0.4891\n",
      "[Rank 0] Epoch 4 | Batch 10 | Loss: 0.4162\n",
      "[Rank 2] Epoch 4 | Batch 10 | Loss: 0.5828\n",
      "[Rank 3] Epoch 4 | Batch 10 | Loss: 0.5379\n",
      "[Rank 2] Epoch 4 | Batch 20 | Loss: 0.4109\n",
      "[Rank 0] Epoch 4 | Batch 20 | Loss: 0.3651\n",
      "[Rank 1] Epoch 4 | Batch 20 | Loss: 0.5669\n",
      "[Rank 3] Epoch 4 | Batch 20 | Loss: 0.3555\n",
      "[Rank 2] Epoch 4 | Batch 30 | Loss: 0.4498\n",
      "[Rank 3] Epoch 4 | Batch 30 | Loss: 0.4874\n",
      "[Rank 1] Epoch 4 | Batch 30 | Loss: 0.4943\n",
      "[Rank 0] Epoch 4 | Batch 30 | Loss: 0.4467\n",
      "[Rank 1] Epoch 4 | Batch 40 | Loss: 0.4582\n",
      "[Rank 3] Epoch 4 | Batch 40 | Loss: 0.4478\n",
      "[Rank 2] Epoch 4 | Batch 40 | Loss: 0.5296\n",
      "[Rank 0] Epoch 4 | Batch 40 | Loss: 0.4433\n",
      "[Rank 1] Epoch 4 | Batch 50 | Loss: 0.5259\n",
      "[Rank 3] Epoch 4 | Batch 50 | Loss: 0.5308\n",
      "[Rank 0] Epoch 4 | Batch 50 | Loss: 0.5447\n",
      "[Rank 2] Epoch 4 | Batch 50 | Loss: 0.2764\n",
      "[Rank 1] Epoch 4 | Batch 60 | Loss: 0.3579\n",
      "[Rank 3] Epoch 4 | Batch 60 | Loss: 0.4247\n",
      "[Rank 0] Epoch 4 | Batch 60 | Loss: 0.4479\n",
      "[Rank 2] Epoch 4 | Batch 60 | Loss: 0.6815\n",
      "[Rank 3] Epoch 4 | Batch 70 | Loss: 0.3037\n",
      "[Rank 2] Epoch 4 | Batch 70 | Loss: 0.3417\n",
      "[Rank 1] Epoch 4 | Batch 70 | Loss: 0.4284\n",
      "[Rank 0] Epoch 4 | Batch 70 | Loss: 0.3564\n",
      "[Rank 2] Epoch 4 | Batch 80 | Loss: 0.3915\n",
      "[Rank 3] Epoch 4 | Batch 80 | Loss: 0.4792\n",
      "[Rank 0] Epoch 4 | Batch 80 | Loss: 0.4654\n",
      "[Rank 1] Epoch 4 | Batch 80 | Loss: 0.4976\n",
      "[Rank 2] Epoch 4 | Batch 90 | Loss: 0.4794\n",
      "[Rank 3] Epoch 4 | Batch 90 | Loss: 0.5508\n",
      "[Rank 0] Epoch 4 | Batch 90 | Loss: 0.3871\n",
      "[Rank 1] Epoch 4 | Batch 90 | Loss: 0.4273\n",
      "[Rank 1] Epoch 4 | Batch 100 | Loss: 0.3036\n",
      "[Rank 3] Epoch 4 | Batch 100 | Loss: 0.3984\n",
      "[Rank 0] Epoch 4 | Batch 100 | Loss: 0.3633\n",
      "[Rank 2] Epoch 4 | Batch 100 | Loss: 0.3909\n",
      "[Rank 1] Epoch 4 | Batch 110 | Loss: 0.3845\n",
      "[Rank 3] Epoch 4 | Batch 110 | Loss: 0.3377\n",
      "[Rank 0] Epoch 4 | Batch 110 | Loss: 0.2260\n",
      "[Rank 2] Epoch 4 | Batch 110 | Loss: 0.3275\n",
      "[Rank 3] Epoch 4 | Batch 120 | Loss: 0.3161\n",
      "[Rank 0] Epoch 4 | Batch 120 | Loss: 0.4148\n",
      "[Rank 2] Epoch 4 | Batch 120 | Loss: 0.4615\n",
      "[Rank 1] Epoch 4 | Batch 120 | Loss: 0.3845\n",
      "[Rank 0] Epoch 4 | Batch 130 | Loss: 0.3842\n",
      "[Rank 1] Epoch 4 | Batch 130 | Loss: 0.3244\n",
      "[Rank 3] Epoch 4 | Batch 130 | Loss: 0.4709\n",
      "[Rank 2] Epoch 4 | Batch 130 | Loss: 0.3029\n",
      "[Rank 0] Epoch 4 | Batch 140 | Loss: 0.5055\n",
      "[Rank 2] Epoch 4 | Batch 140 | Loss: 0.3339\n",
      "[Rank 3] Epoch 4 | Batch 140 | Loss: 0.6956\n",
      "[Rank 1] Epoch 4 | Batch 140 | Loss: 0.4442\n",
      "[Rank 0] Epoch 4 | Batch 150 | Loss: 0.6393\n",
      "[Rank 2] Epoch 4 | Batch 150 | Loss: 0.5208\n",
      "[Rank 3] Epoch 4 | Batch 150 | Loss: 0.4188\n",
      "[Rank 1] Epoch 4 | Batch 150 | Loss: 0.4287\n",
      "[Rank 3] Epoch 4 | Batch 160 | Loss: 0.5632\n",
      "[Rank 2] Epoch 4 | Batch 160 | Loss: 0.3278\n",
      "[Rank 0] Epoch 4 | Batch 160 | Loss: 0.5387\n",
      "[Rank 1] Epoch 4 | Batch 160 | Loss: 0.3061\n",
      "[Rank 2] Epoch 4 | Batch 170 | Loss: 0.3285\n",
      "[Rank 1] Epoch 4 | Batch 170 | Loss: 0.4771\n",
      "[Rank 3] Epoch 4 | Batch 170 | Loss: 0.4045\n",
      "[Rank 0] Epoch 4 | Batch 170 | Loss: 0.4428\n",
      "[Rank 2] Epoch 4 | Batch 180 | Loss: 0.3615\n",
      "[Rank 3] Epoch 4 | Batch 180 | Loss: 0.2996\n",
      "[Rank 0] Epoch 4 | Batch 180 | Loss: 0.4137\n",
      "[Rank 1] Epoch 4 | Batch 180 | Loss: 0.3760\n",
      "[Rank 1] Epoch 4 | Batch 190 | Loss: 0.6185\n",
      "[Rank 2] Epoch 4 | Batch 190 | Loss: 0.4451\n",
      "[Rank 3] Epoch 4 | Batch 190 | Loss: 0.3390\n",
      "[Rank 0] Epoch 4 | Batch 190 | Loss: 0.4317\n",
      "[Rank 3] Epoch 4 | Batch 200 | Loss: 0.4728\n",
      "[Rank 0] Epoch 4 | Batch 200 | Loss: 0.4120\n",
      "[Rank 1] Epoch 4 | Batch 200 | Loss: 0.4310\n",
      "[Rank 2] Epoch 4 | Batch 200 | Loss: 0.3395\n",
      "⏱️ [Rank 3] Epoch 4 completed in 466.85 seconds\n",
      "⏱️ [Rank 1] Epoch 4 completed in 467.24 seconds\n",
      "⏱️ [Rank 2] Epoch 4 completed in 467.13 seconds\n",
      "⏱️ [Rank 0] Epoch 4 completed in 467.12 seconds\n",
      "[Rank 3] Epoch 5 | Batch 0 | Loss: 0.3389\n",
      "[Rank 2] Epoch 5 | Batch 0 | Loss: 0.3598\n",
      "[Rank 0] Epoch 5 | Batch 0 | Loss: 0.5292\n",
      "[Rank 1] Epoch 5 | Batch 0 | Loss: 0.4843\n",
      "[Rank 1] Epoch 5 | Batch 10 | Loss: 0.4750\n",
      "[Rank 0] Epoch 5 | Batch 10 | Loss: 0.2651\n",
      "[Rank 2] Epoch 5 | Batch 10 | Loss: 0.3992\n",
      "[Rank 3] Epoch 5 | Batch 10 | Loss: 0.4536\n",
      "[Rank 2] Epoch 5 | Batch 20 | Loss: 0.4257\n",
      "[Rank 0] Epoch 5 | Batch 20 | Loss: 0.3638\n",
      "[Rank 1] Epoch 5 | Batch 20 | Loss: 0.4014\n",
      "[Rank 3] Epoch 5 | Batch 20 | Loss: 0.3956\n",
      "[Rank 1] Epoch 5 | Batch 30 | Loss: 0.3870\n",
      "[Rank 2] Epoch 5 | Batch 30 | Loss: 0.2360\n",
      "[Rank 0] Epoch 5 | Batch 30 | Loss: 0.3801\n",
      "[Rank 3] Epoch 5 | Batch 30 | Loss: 0.3477\n",
      "[Rank 1] Epoch 5 | Batch 40 | Loss: 0.3934\n",
      "[Rank 2] Epoch 5 | Batch 40 | Loss: 0.2989\n",
      "[Rank 3] Epoch 5 | Batch 40 | Loss: 0.4196\n",
      "[Rank 0] Epoch 5 | Batch 40 | Loss: 0.3736\n",
      "[Rank 1] Epoch 5 | Batch 50 | Loss: 0.5179\n",
      "[Rank 3] Epoch 5 | Batch 50 | Loss: 0.4032\n",
      "[Rank 0] Epoch 5 | Batch 50 | Loss: 0.3808\n",
      "[Rank 2] Epoch 5 | Batch 50 | Loss: 0.3356\n",
      "[Rank 0] Epoch 5 | Batch 60 | Loss: 0.3271\n",
      "[Rank 3] Epoch 5 | Batch 60 | Loss: 0.3959\n",
      "[Rank 1] Epoch 5 | Batch 60 | Loss: 0.2400\n",
      "[Rank 2] Epoch 5 | Batch 60 | Loss: 0.3628\n",
      "[Rank 0] Epoch 5 | Batch 70 | Loss: 0.5099\n",
      "[Rank 3] Epoch 5 | Batch 70 | Loss: 0.4949\n",
      "[Rank 2] Epoch 5 | Batch 70 | Loss: 0.3469\n",
      "[Rank 1] Epoch 5 | Batch 70 | Loss: 0.5205\n",
      "[Rank 0] Epoch 5 | Batch 80 | Loss: 0.4605\n",
      "[Rank 2] Epoch 5 | Batch 80 | Loss: 0.4394\n",
      "[Rank 3] Epoch 5 | Batch 80 | Loss: 0.4772\n",
      "[Rank 1] Epoch 5 | Batch 80 | Loss: 0.3590\n",
      "[Rank 0] Epoch 5 | Batch 90 | Loss: 0.6959\n",
      "[Rank 1] Epoch 5 | Batch 90 | Loss: 0.4287\n",
      "[Rank 3] Epoch 5 | Batch 90 | Loss: 0.5599\n",
      "[Rank 2] Epoch 5 | Batch 90 | Loss: 0.3498\n",
      "[Rank 0] Epoch 5 | Batch 100 | Loss: 0.2803\n",
      "[Rank 1] Epoch 5 | Batch 100 | Loss: 0.4574\n",
      "[Rank 3] Epoch 5 | Batch 100 | Loss: 0.4022\n",
      "[Rank 2] Epoch 5 | Batch 100 | Loss: 0.3497\n",
      "[Rank 3] Epoch 5 | Batch 110 | Loss: 0.3211\n",
      "[Rank 1] Epoch 5 | Batch 110 | Loss: 0.3781\n",
      "[Rank 0] Epoch 5 | Batch 110 | Loss: 0.3651\n",
      "[Rank 2] Epoch 5 | Batch 110 | Loss: 0.4174\n",
      "[Rank 3] Epoch 5 | Batch 120 | Loss: 0.4516\n",
      "[Rank 1] Epoch 5 | Batch 120 | Loss: 0.4814\n",
      "[Rank 0] Epoch 5 | Batch 120 | Loss: 0.2506\n",
      "[Rank 2] Epoch 5 | Batch 120 | Loss: 0.4023\n",
      "[Rank 0] Epoch 5 | Batch 130 | Loss: 0.4242\n",
      "[Rank 3] Epoch 5 | Batch 130 | Loss: 0.4795\n",
      "[Rank 1] Epoch 5 | Batch 130 | Loss: 0.4630\n",
      "[Rank 2] Epoch 5 | Batch 130 | Loss: 0.3938\n",
      "[Rank 1] Epoch 5 | Batch 140 | Loss: 0.3157\n",
      "[Rank 3] Epoch 5 | Batch 140 | Loss: 0.4784\n",
      "[Rank 0] Epoch 5 | Batch 140 | Loss: 0.4014\n",
      "[Rank 2] Epoch 5 | Batch 140 | Loss: 0.5519\n",
      "[Rank 0] Epoch 5 | Batch 150 | Loss: 0.5862\n",
      "[Rank 3] Epoch 5 | Batch 150 | Loss: 0.3025\n",
      "[Rank 1] Epoch 5 | Batch 150 | Loss: 0.4670\n",
      "[Rank 2] Epoch 5 | Batch 150 | Loss: 0.4779\n",
      "[Rank 0] Epoch 5 | Batch 160 | Loss: 0.3871\n",
      "[Rank 3] Epoch 5 | Batch 160 | Loss: 0.2629\n",
      "[Rank 2] Epoch 5 | Batch 160 | Loss: 0.4280\n",
      "[Rank 1] Epoch 5 | Batch 160 | Loss: 0.2364\n",
      "[Rank 2] Epoch 5 | Batch 170 | Loss: 0.4362\n",
      "[Rank 1] Epoch 5 | Batch 170 | Loss: 0.4200\n",
      "[Rank 0] Epoch 5 | Batch 170 | Loss: 0.3549\n",
      "[Rank 3] Epoch 5 | Batch 170 | Loss: 0.5006\n",
      "[Rank 2] Epoch 5 | Batch 180 | Loss: 0.3309\n",
      "[Rank 1] Epoch 5 | Batch 180 | Loss: 0.5214\n",
      "[Rank 0] Epoch 5 | Batch 180 | Loss: 0.3682\n",
      "[Rank 3] Epoch 5 | Batch 180 | Loss: 0.3259\n",
      "[Rank 2] Epoch 5 | Batch 190 | Loss: 0.4393\n",
      "[Rank 1] Epoch 5 | Batch 190 | Loss: 0.3952\n",
      "[Rank 0] Epoch 5 | Batch 190 | Loss: 0.3693\n",
      "[Rank 3] Epoch 5 | Batch 190 | Loss: 0.4690\n",
      "[Rank 1] Epoch 5 | Batch 200 | Loss: 0.6113\n",
      "[Rank 0] Epoch 5 | Batch 200 | Loss: 0.5317\n",
      "[Rank 2] Epoch 5 | Batch 200 | Loss: 0.3726\n",
      "[Rank 3] Epoch 5 | Batch 200 | Loss: 0.3096\n",
      "⏱️ [Rank 3] Epoch 5 completed in 485.41 seconds\n",
      "⏱️ [Rank 1] Epoch 5 completed in 485.25 seconds\n",
      "⏱️ [Rank 0] Epoch 5 completed in 485.19 seconds\n",
      "⏱️ [Rank 2] Epoch 5 completed in 485.30 seconds\n",
      "✅ Model saved as cnn_ddp_model.pth\n"
     ]
    }
   ],
   "source": [
    "!python ddp_train_cpu.py \\\n",
    "  --image-dir \"/scratch/mohammed.moi/yolo_workspace/pneumonia_dataset/Pneumonia Dataset/Training/Images\" \\\n",
    "  --mask-dir \"/scratch/mohammed.moi/yolo_workspace/pneumonia_dataset/Pneumonia Dataset/Training/Masks\" \\\n",
    "  --save-dir \"/scratch/mohammed.moi/yolo_workspace\" \\\n",
    "  --world-size 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9310c2d2-5f9b-481d-8eb6-a2494dc4f41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mohammed.moi/ddp_train_cpu.py:111: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target = torch.tensor(target).to(device)\n",
      "/home/mohammed.moi/ddp_train_cpu.py:111: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target = torch.tensor(target).to(device)\n",
      "/home/mohammed.moi/ddp_train_cpu.py:111: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target = torch.tensor(target).to(device)\n",
      "/home/mohammed.moi/ddp_train_cpu.py:111: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target = torch.tensor(target).to(device)\n",
      "[Rank 1] Epoch 1 | Batch 0 | Loss: 0.7297\n",
      "[Rank 0] Epoch 1 | Batch 0 | Loss: 0.7302\n",
      "[Rank 2] Epoch 1 | Batch 0 | Loss: 0.7149\n",
      "[Rank 3] Epoch 1 | Batch 0 | Loss: 0.7236\n",
      "[Rank 0] Epoch 1 | Batch 10 | Loss: 0.7136\n",
      "[Rank 3] Epoch 1 | Batch 10 | Loss: 0.7267\n",
      "[Rank 1] Epoch 1 | Batch 10 | Loss: 0.7185\n",
      "[Rank 2] Epoch 1 | Batch 10 | Loss: 0.7308\n",
      "[Rank 3] Epoch 1 | Batch 20 | Loss: 0.4490\n",
      "[Rank 0] Epoch 1 | Batch 20 | Loss: 0.5279\n",
      "[Rank 2] Epoch 1 | Batch 20 | Loss: 0.5599\n",
      "[Rank 1] Epoch 1 | Batch 20 | Loss: 0.5257\n",
      "[Rank 3] Epoch 1 | Batch 30 | Loss: 0.4579\n",
      "[Rank 1] Epoch 1 | Batch 30 | Loss: 0.5314\n",
      "[Rank 0] Epoch 1 | Batch 30 | Loss: 0.5079\n",
      "[Rank 2] Epoch 1 | Batch 30 | Loss: 0.5509\n",
      "[Rank 2] Epoch 1 | Batch 40 | Loss: 0.5575\n",
      "[Rank 0] Epoch 1 | Batch 40 | Loss: 0.5383\n",
      "[Rank 3] Epoch 1 | Batch 40 | Loss: 0.5562\n",
      "[Rank 1] Epoch 1 | Batch 40 | Loss: 0.5138\n",
      "[Rank 3] Epoch 1 | Batch 50 | Loss: 0.6395\n",
      "[Rank 0] Epoch 1 | Batch 50 | Loss: 0.4194\n",
      "[Rank 1] Epoch 1 | Batch 50 | Loss: 0.5887\n",
      "[Rank 2] Epoch 1 | Batch 50 | Loss: 0.5186\n",
      "[Rank 3] Epoch 1 | Batch 60 | Loss: 0.5329\n",
      "[Rank 1] Epoch 1 | Batch 60 | Loss: 0.5900\n",
      "[Rank 0] Epoch 1 | Batch 60 | Loss: 0.4587\n",
      "[Rank 2] Epoch 1 | Batch 60 | Loss: 0.4151\n",
      "[Rank 1] Epoch 1 | Batch 70 | Loss: 0.4416\n",
      "[Rank 2] Epoch 1 | Batch 70 | Loss: 0.4027\n",
      "[Rank 0] Epoch 1 | Batch 70 | Loss: 0.2913\n",
      "[Rank 3] Epoch 1 | Batch 70 | Loss: 0.3855\n",
      "[Rank 1] Epoch 1 | Batch 80 | Loss: 0.4201\n",
      "[Rank 0] Epoch 1 | Batch 80 | Loss: 0.4235\n",
      "[Rank 2] Epoch 1 | Batch 80 | Loss: 0.3462\n",
      "[Rank 3] Epoch 1 | Batch 80 | Loss: 0.4057\n",
      "[Rank 0] Epoch 1 | Batch 90 | Loss: 0.7383\n",
      "[Rank 3] Epoch 1 | Batch 90 | Loss: 0.4416\n",
      "[Rank 1] Epoch 1 | Batch 90 | Loss: 0.3050\n",
      "[Rank 2] Epoch 1 | Batch 90 | Loss: 0.4531\n",
      "[Rank 1] Epoch 1 | Batch 100 | Loss: 0.5382\n",
      "[Rank 3] Epoch 1 | Batch 100 | Loss: 0.5416\n",
      "[Rank 2] Epoch 1 | Batch 100 | Loss: 0.4177\n",
      "[Rank 0] Epoch 1 | Batch 100 | Loss: 0.4772\n",
      "[Rank 2] Epoch 1 | Batch 110 | Loss: 0.4707\n",
      "[Rank 0] Epoch 1 | Batch 110 | Loss: 0.3742\n",
      "[Rank 1] Epoch 1 | Batch 110 | Loss: 0.2495\n",
      "[Rank 3] Epoch 1 | Batch 110 | Loss: 0.3907\n",
      "[Rank 0] Epoch 1 | Batch 120 | Loss: 0.4173\n",
      "[Rank 3] Epoch 1 | Batch 120 | Loss: 0.3727\n",
      "[Rank 2] Epoch 1 | Batch 120 | Loss: 0.4926\n",
      "[Rank 1] Epoch 1 | Batch 120 | Loss: 0.6697\n",
      "[Rank 2] Epoch 1 | Batch 130 | Loss: 0.3586\n",
      "[Rank 3] Epoch 1 | Batch 130 | Loss: 0.6099\n",
      "[Rank 1] Epoch 1 | Batch 130 | Loss: 0.4478\n",
      "[Rank 0] Epoch 1 | Batch 130 | Loss: 0.5728\n",
      "[Rank 2] Epoch 1 | Batch 140 | Loss: 0.2867\n",
      "[Rank 0] Epoch 1 | Batch 140 | Loss: 0.3545\n",
      "[Rank 1] Epoch 1 | Batch 140 | Loss: 0.3932\n",
      "[Rank 3] Epoch 1 | Batch 140 | Loss: 0.2677\n",
      "[Rank 3] Epoch 1 | Batch 150 | Loss: 0.5232\n",
      "[Rank 0] Epoch 1 | Batch 150 | Loss: 0.5106\n",
      "[Rank 1] Epoch 1 | Batch 150 | Loss: 0.4287\n",
      "[Rank 2] Epoch 1 | Batch 150 | Loss: 0.3449\n",
      "[Rank 2] Epoch 1 | Batch 160 | Loss: 0.4616\n",
      "[Rank 1] Epoch 1 | Batch 160 | Loss: 0.3283\n",
      "[Rank 3] Epoch 1 | Batch 160 | Loss: 0.4687\n",
      "[Rank 0] Epoch 1 | Batch 160 | Loss: 0.4552\n",
      "[Rank 1] Epoch 1 | Batch 170 | Loss: 0.4290\n",
      "[Rank 2] Epoch 1 | Batch 170 | Loss: 0.4043\n",
      "[Rank 3] Epoch 1 | Batch 170 | Loss: 0.5663\n",
      "[Rank 0] Epoch 1 | Batch 170 | Loss: 0.4982\n",
      "[Rank 1] Epoch 1 | Batch 180 | Loss: 0.3591\n",
      "[Rank 3] Epoch 1 | Batch 180 | Loss: 0.6810\n",
      "[Rank 0] Epoch 1 | Batch 180 | Loss: 0.2702\n",
      "[Rank 2] Epoch 1 | Batch 180 | Loss: 0.3234\n",
      "[Rank 0] Epoch 1 | Batch 190 | Loss: 0.6510\n",
      "[Rank 3] Epoch 1 | Batch 190 | Loss: 0.5144\n",
      "[Rank 2] Epoch 1 | Batch 190 | Loss: 0.3706\n",
      "[Rank 1] Epoch 1 | Batch 190 | Loss: 0.4606\n",
      "[Rank 2] Epoch 1 | Batch 200 | Loss: 0.5261\n",
      "[Rank 1] Epoch 1 | Batch 200 | Loss: 0.4052\n",
      "[Rank 0] Epoch 1 | Batch 200 | Loss: 0.5083\n",
      "[Rank 3] Epoch 1 | Batch 200 | Loss: 0.3245\n",
      "⏱️ [Rank 0] Epoch 1 completed in 465.86 seconds\n",
      "⏳ [Rank 0] Epoch 1 | Training Loss: 0.5361 | Accuracy: 76.29%\n",
      "⏱️ [Rank 1] Epoch 1 completed in 465.86 seconds\n",
      "⏳ [Rank 1] Epoch 1 | Training Loss: 0.5247 | Accuracy: 76.48%\n",
      "⏱️ [Rank 2] Epoch 1 completed in 465.94 seconds\n",
      "⏳ [Rank 2] Epoch 1 | Training Loss: 0.5642 | Accuracy: 75.82%\n",
      "⏱️ [Rank 3] Epoch 1 completed in 466.03 seconds\n",
      "⏳ [Rank 3] Epoch 1 | Training Loss: 0.5456 | Accuracy: 75.61%\n",
      "[Rank 0] Epoch 2 | Batch 0 | Loss: 0.4109\n",
      "[Rank 2] Epoch 2 | Batch 0 | Loss: 0.3176\n",
      "[Rank 3] Epoch 2 | Batch 0 | Loss: 0.5826\n",
      "[Rank 1] Epoch 2 | Batch 0 | Loss: 0.5784\n",
      "[Rank 2] Epoch 2 | Batch 10 | Loss: 0.2766\n",
      "[Rank 0] Epoch 2 | Batch 10 | Loss: 0.4069\n",
      "[Rank 1] Epoch 2 | Batch 10 | Loss: 0.6232\n",
      "[Rank 3] Epoch 2 | Batch 10 | Loss: 0.4156\n",
      "[Rank 3] Epoch 2 | Batch 20 | Loss: 0.3942\n",
      "[Rank 2] Epoch 2 | Batch 20 | Loss: 0.4331\n",
      "[Rank 0] Epoch 2 | Batch 20 | Loss: 0.4152\n",
      "[Rank 1] Epoch 2 | Batch 20 | Loss: 0.4172\n",
      "[Rank 2] Epoch 2 | Batch 30 | Loss: 0.6475\n",
      "[Rank 3] Epoch 2 | Batch 30 | Loss: 0.5472\n",
      "[Rank 0] Epoch 2 | Batch 30 | Loss: 0.6342\n",
      "[Rank 1] Epoch 2 | Batch 30 | Loss: 0.4279\n",
      "[Rank 1] Epoch 2 | Batch 40 | Loss: 0.3675\n",
      "[Rank 2] Epoch 2 | Batch 40 | Loss: 0.4312\n",
      "[Rank 3] Epoch 2 | Batch 40 | Loss: 0.4924\n",
      "[Rank 0] Epoch 2 | Batch 40 | Loss: 0.3650\n",
      "[Rank 3] Epoch 2 | Batch 50 | Loss: 0.4422\n",
      "[Rank 2] Epoch 2 | Batch 50 | Loss: 0.2738\n",
      "[Rank 1] Epoch 2 | Batch 50 | Loss: 0.2680\n",
      "[Rank 0] Epoch 2 | Batch 50 | Loss: 0.3508\n",
      "[Rank 2] Epoch 2 | Batch 60 | Loss: 0.4379\n",
      "[Rank 1] Epoch 2 | Batch 60 | Loss: 0.4606\n",
      "[Rank 3] Epoch 2 | Batch 60 | Loss: 0.4183\n",
      "[Rank 0] Epoch 2 | Batch 60 | Loss: 0.5210\n",
      "[Rank 1] Epoch 2 | Batch 70 | Loss: 0.4187\n",
      "[Rank 0] Epoch 2 | Batch 70 | Loss: 0.3350\n",
      "[Rank 2] Epoch 2 | Batch 70 | Loss: 0.6419\n",
      "[Rank 3] Epoch 2 | Batch 70 | Loss: 0.3877\n",
      "[Rank 0] Epoch 2 | Batch 80 | Loss: 0.6061\n",
      "[Rank 1] Epoch 2 | Batch 80 | Loss: 0.4793\n",
      "[Rank 3] Epoch 2 | Batch 80 | Loss: 0.3110\n",
      "[Rank 2] Epoch 2 | Batch 80 | Loss: 0.5825\n",
      "[Rank 0] Epoch 2 | Batch 90 | Loss: 0.2620\n",
      "[Rank 3] Epoch 2 | Batch 90 | Loss: 0.5235\n",
      "[Rank 1] Epoch 2 | Batch 90 | Loss: 0.4714\n",
      "[Rank 2] Epoch 2 | Batch 90 | Loss: 0.2927\n",
      "[Rank 1] Epoch 2 | Batch 100 | Loss: 0.4045\n",
      "[Rank 0] Epoch 2 | Batch 100 | Loss: 0.6154\n",
      "[Rank 2] Epoch 2 | Batch 100 | Loss: 0.5291\n",
      "[Rank 3] Epoch 2 | Batch 100 | Loss: 0.3521\n",
      "[Rank 3] Epoch 2 | Batch 110 | Loss: 0.4221\n",
      "[Rank 1] Epoch 2 | Batch 110 | Loss: 0.5208\n",
      "[Rank 2] Epoch 2 | Batch 110 | Loss: 0.4676\n",
      "[Rank 0] Epoch 2 | Batch 110 | Loss: 0.4570\n",
      "[Rank 2] Epoch 2 | Batch 120 | Loss: 0.4080\n",
      "[Rank 1] Epoch 2 | Batch 120 | Loss: 0.4157\n",
      "[Rank 0] Epoch 2 | Batch 120 | Loss: 0.4470\n",
      "[Rank 3] Epoch 2 | Batch 120 | Loss: 0.4295\n",
      "[Rank 2] Epoch 2 | Batch 130 | Loss: 0.2566\n",
      "[Rank 1] Epoch 2 | Batch 130 | Loss: 0.3422\n",
      "[Rank 3] Epoch 2 | Batch 130 | Loss: 0.5460\n",
      "[Rank 0] Epoch 2 | Batch 130 | Loss: 0.4173\n",
      "[Rank 1] Epoch 2 | Batch 140 | Loss: 0.3950\n",
      "[Rank 0] Epoch 2 | Batch 140 | Loss: 0.6023\n",
      "[Rank 2] Epoch 2 | Batch 140 | Loss: 0.4850\n",
      "[Rank 3] Epoch 2 | Batch 140 | Loss: 0.4642\n",
      "[Rank 2] Epoch 2 | Batch 150 | Loss: 0.4128\n",
      "[Rank 3] Epoch 2 | Batch 150 | Loss: 0.3455\n",
      "[Rank 0] Epoch 2 | Batch 150 | Loss: 0.5677\n",
      "[Rank 1] Epoch 2 | Batch 150 | Loss: 0.4108\n",
      "[Rank 1] Epoch 2 | Batch 160 | Loss: 0.3890\n",
      "[Rank 3] Epoch 2 | Batch 160 | Loss: 0.5372\n",
      "[Rank 0] Epoch 2 | Batch 160 | Loss: 0.5463\n",
      "[Rank 2] Epoch 2 | Batch 160 | Loss: 0.3074\n",
      "[Rank 1] Epoch 2 | Batch 170 | Loss: 0.2427\n",
      "[Rank 0] Epoch 2 | Batch 170 | Loss: 0.4054\n",
      "[Rank 3] Epoch 2 | Batch 170 | Loss: 0.5650\n",
      "[Rank 2] Epoch 2 | Batch 170 | Loss: 0.5833\n",
      "[Rank 0] Epoch 2 | Batch 180 | Loss: 0.4482\n",
      "[Rank 2] Epoch 2 | Batch 180 | Loss: 0.3009\n",
      "[Rank 3] Epoch 2 | Batch 180 | Loss: 0.3992\n",
      "[Rank 1] Epoch 2 | Batch 180 | Loss: 0.3638\n",
      "[Rank 3] Epoch 2 | Batch 190 | Loss: 0.4378\n",
      "[Rank 0] Epoch 2 | Batch 190 | Loss: 0.5604\n",
      "[Rank 1] Epoch 2 | Batch 190 | Loss: 0.4119\n",
      "[Rank 2] Epoch 2 | Batch 190 | Loss: 0.3107\n",
      "[Rank 3] Epoch 2 | Batch 200 | Loss: 0.5969\n",
      "[Rank 0] Epoch 2 | Batch 200 | Loss: 0.4752\n",
      "[Rank 2] Epoch 2 | Batch 200 | Loss: 0.3166\n",
      "[Rank 1] Epoch 2 | Batch 200 | Loss: 0.6070\n",
      "⏱️ [Rank 3] Epoch 2 completed in 465.94 seconds\n",
      "⏳ [Rank 3] Epoch 2 | Training Loss: 0.4402 | Accuracy: 79.51%\n",
      "⏱️ [Rank 1] Epoch 2 completed in 466.44 seconds\n",
      "⏳ [Rank 1] Epoch 2 | Training Loss: 0.4342 | Accuracy: 79.99%\n",
      "⏱️ [Rank 2] Epoch 2 completed in 466.38 seconds\n",
      "⏳ [Rank 2] Epoch 2 | Training Loss: 0.4332 | Accuracy: 79.93%\n",
      "⏱️ [Rank 0] Epoch 2 completed in 466.46 seconds\n",
      "⏳ [Rank 0] Epoch 2 | Training Loss: 0.4404 | Accuracy: 79.61%\n",
      "[Rank 2] Epoch 3 | Batch 0 | Loss: 0.4845\n",
      "[Rank 1] Epoch 3 | Batch 0 | Loss: 0.2281\n",
      "[Rank 3] Epoch 3 | Batch 0 | Loss: 0.4637\n",
      "[Rank 0] Epoch 3 | Batch 0 | Loss: 0.3646\n",
      "[Rank 0] Epoch 3 | Batch 10 | Loss: 0.4070\n",
      "[Rank 1] Epoch 3 | Batch 10 | Loss: 0.4883\n",
      "[Rank 3] Epoch 3 | Batch 10 | Loss: 0.3616\n",
      "[Rank 2] Epoch 3 | Batch 10 | Loss: 0.3975\n",
      "[Rank 1] Epoch 3 | Batch 20 | Loss: 0.4871\n",
      "[Rank 2] Epoch 3 | Batch 20 | Loss: 0.6604\n",
      "[Rank 0] Epoch 3 | Batch 20 | Loss: 0.5263\n",
      "[Rank 3] Epoch 3 | Batch 20 | Loss: 0.5400\n",
      "[Rank 1] Epoch 3 | Batch 30 | Loss: 0.5443\n",
      "[Rank 3] Epoch 3 | Batch 30 | Loss: 0.3335\n",
      "[Rank 0] Epoch 3 | Batch 30 | Loss: 0.4796\n",
      "[Rank 2] Epoch 3 | Batch 30 | Loss: 0.2621\n",
      "[Rank 3] Epoch 3 | Batch 40 | Loss: 0.4343\n",
      "[Rank 1] Epoch 3 | Batch 40 | Loss: 0.5501\n",
      "[Rank 0] Epoch 3 | Batch 40 | Loss: 0.4994\n",
      "[Rank 2] Epoch 3 | Batch 40 | Loss: 0.5572\n",
      "[Rank 2] Epoch 3 | Batch 50 | Loss: 0.3376\n",
      "[Rank 0] Epoch 3 | Batch 50 | Loss: 0.4932\n",
      "[Rank 3] Epoch 3 | Batch 50 | Loss: 0.4223\n",
      "[Rank 1] Epoch 3 | Batch 50 | Loss: 0.3663\n",
      "[Rank 2] Epoch 3 | Batch 60 | Loss: 0.5692\n",
      "[Rank 0] Epoch 3 | Batch 60 | Loss: 0.3945\n",
      "[Rank 3] Epoch 3 | Batch 60 | Loss: 0.5916\n",
      "[Rank 1] Epoch 3 | Batch 60 | Loss: 0.4862\n",
      "[Rank 0] Epoch 3 | Batch 70 | Loss: 0.4906\n",
      "[Rank 3] Epoch 3 | Batch 70 | Loss: 0.4134\n",
      "[Rank 2] Epoch 3 | Batch 70 | Loss: 0.4678\n",
      "[Rank 1] Epoch 3 | Batch 70 | Loss: 0.5525\n",
      "[Rank 1] Epoch 3 | Batch 80 | Loss: 0.5592\n",
      "[Rank 0] Epoch 3 | Batch 80 | Loss: 0.3691\n",
      "[Rank 3] Epoch 3 | Batch 80 | Loss: 0.3000\n",
      "[Rank 2] Epoch 3 | Batch 80 | Loss: 0.4317\n",
      "[Rank 0] Epoch 3 | Batch 90 | Loss: 0.4645\n",
      "[Rank 3] Epoch 3 | Batch 90 | Loss: 0.4324\n",
      "[Rank 2] Epoch 3 | Batch 90 | Loss: 0.4141\n",
      "[Rank 1] Epoch 3 | Batch 90 | Loss: 0.3577\n",
      "[Rank 3] Epoch 3 | Batch 100 | Loss: 0.5233\n",
      "[Rank 0] Epoch 3 | Batch 100 | Loss: 0.2890\n",
      "[Rank 2] Epoch 3 | Batch 100 | Loss: 0.2921\n",
      "[Rank 1] Epoch 3 | Batch 100 | Loss: 0.4229\n",
      "[Rank 1] Epoch 3 | Batch 110 | Loss: 0.3670\n",
      "[Rank 0] Epoch 3 | Batch 110 | Loss: 0.3665\n",
      "[Rank 2] Epoch 3 | Batch 110 | Loss: 0.5363\n",
      "[Rank 3] Epoch 3 | Batch 110 | Loss: 0.3080\n",
      "[Rank 0] Epoch 3 | Batch 120 | Loss: 0.4496\n",
      "[Rank 1] Epoch 3 | Batch 120 | Loss: 0.4739\n",
      "[Rank 3] Epoch 3 | Batch 120 | Loss: 0.3345\n",
      "[Rank 2] Epoch 3 | Batch 120 | Loss: 0.3228\n",
      "[Rank 0] Epoch 3 | Batch 130 | Loss: 0.3494\n",
      "[Rank 3] Epoch 3 | Batch 130 | Loss: 0.4635\n",
      "[Rank 1] Epoch 3 | Batch 130 | Loss: 0.5719\n",
      "[Rank 2] Epoch 3 | Batch 130 | Loss: 0.2901\n",
      "[Rank 2] Epoch 3 | Batch 140 | Loss: 0.4769\n",
      "[Rank 3] Epoch 3 | Batch 140 | Loss: 0.4403\n",
      "[Rank 1] Epoch 3 | Batch 140 | Loss: 0.4740\n",
      "[Rank 0] Epoch 3 | Batch 140 | Loss: 0.4352\n",
      "[Rank 0] Epoch 3 | Batch 150 | Loss: 0.4113\n",
      "[Rank 2] Epoch 3 | Batch 150 | Loss: 0.4195\n",
      "[Rank 3] Epoch 3 | Batch 150 | Loss: 0.5949\n",
      "[Rank 1] Epoch 3 | Batch 150 | Loss: 0.3687\n",
      "[Rank 1] Epoch 3 | Batch 160 | Loss: 0.5987\n",
      "[Rank 2] Epoch 3 | Batch 160 | Loss: 0.2686\n",
      "[Rank 0] Epoch 3 | Batch 160 | Loss: 0.3860\n",
      "[Rank 3] Epoch 3 | Batch 160 | Loss: 0.5933\n",
      "[Rank 2] Epoch 3 | Batch 170 | Loss: 0.5019\n",
      "[Rank 0] Epoch 3 | Batch 170 | Loss: 0.4758\n",
      "[Rank 1] Epoch 3 | Batch 170 | Loss: 0.5516\n",
      "[Rank 3] Epoch 3 | Batch 170 | Loss: 0.4144\n",
      "[Rank 0] Epoch 3 | Batch 180 | Loss: 0.4849\n",
      "[Rank 2] Epoch 3 | Batch 180 | Loss: 0.4039\n",
      "[Rank 3] Epoch 3 | Batch 180 | Loss: 0.3819\n",
      "[Rank 1] Epoch 3 | Batch 180 | Loss: 0.3488\n",
      "[Rank 1] Epoch 3 | Batch 190 | Loss: 0.5292\n",
      "[Rank 3] Epoch 3 | Batch 190 | Loss: 0.3348\n",
      "[Rank 0] Epoch 3 | Batch 190 | Loss: 0.4289\n",
      "[Rank 2] Epoch 3 | Batch 190 | Loss: 0.4432\n",
      "[Rank 0] Epoch 3 | Batch 200 | Loss: 0.3512\n",
      "[Rank 2] Epoch 3 | Batch 200 | Loss: 0.5614\n",
      "[Rank 1] Epoch 3 | Batch 200 | Loss: 0.4441\n",
      "[Rank 3] Epoch 3 | Batch 200 | Loss: 0.4042\n",
      "⏱️ [Rank 3] Epoch 3 completed in 481.47 seconds\n",
      "⏳ [Rank 3] Epoch 3 | Training Loss: 0.4234 | Accuracy: 79.94%\n",
      "⏱️ [Rank 2] Epoch 3 completed in 481.18 seconds\n",
      "⏳ [Rank 2] Epoch 3 | Training Loss: 0.4368 | Accuracy: 79.82%\n",
      "⏱️ [Rank 0] Epoch 3 completed in 481.19 seconds\n",
      "⏳ [Rank 0] Epoch 3 | Training Loss: 0.4273 | Accuracy: 80.33%\n",
      "⏱️ [Rank 1] Epoch 3 completed in 481.28 seconds\n",
      "⏳ [Rank 1] Epoch 3 | Training Loss: 0.4410 | Accuracy: 79.70%\n",
      "[Rank 0] Epoch 4 | Batch 0 | Loss: 0.3917\n",
      "[Rank 2] Epoch 4 | Batch 0 | Loss: 0.3800\n",
      "[Rank 1] Epoch 4 | Batch 0 | Loss: 0.4926\n",
      "[Rank 3] Epoch 4 | Batch 0 | Loss: 0.3993\n",
      "[Rank 2] Epoch 4 | Batch 10 | Loss: 0.5828\n",
      "[Rank 0] Epoch 4 | Batch 10 | Loss: 0.4162\n",
      "[Rank 1] Epoch 4 | Batch 10 | Loss: 0.4891\n",
      "[Rank 3] Epoch 4 | Batch 10 | Loss: 0.5379\n",
      "[Rank 0] Epoch 4 | Batch 20 | Loss: 0.3651\n",
      "[Rank 2] Epoch 4 | Batch 20 | Loss: 0.4109\n",
      "[Rank 3] Epoch 4 | Batch 20 | Loss: 0.3555\n",
      "[Rank 1] Epoch 4 | Batch 20 | Loss: 0.5669\n",
      "[Rank 1] Epoch 4 | Batch 30 | Loss: 0.4943\n",
      "[Rank 0] Epoch 4 | Batch 30 | Loss: 0.4467\n",
      "[Rank 2] Epoch 4 | Batch 30 | Loss: 0.4498\n",
      "[Rank 3] Epoch 4 | Batch 30 | Loss: 0.4874\n",
      "[Rank 3] Epoch 4 | Batch 40 | Loss: 0.4478\n",
      "[Rank 1] Epoch 4 | Batch 40 | Loss: 0.4582\n",
      "[Rank 2] Epoch 4 | Batch 40 | Loss: 0.5296\n",
      "[Rank 0] Epoch 4 | Batch 40 | Loss: 0.4433\n",
      "[Rank 0] Epoch 4 | Batch 50 | Loss: 0.5447\n",
      "[Rank 3] Epoch 4 | Batch 50 | Loss: 0.5308\n",
      "[Rank 2] Epoch 4 | Batch 50 | Loss: 0.2764\n",
      "[Rank 1] Epoch 4 | Batch 50 | Loss: 0.5259\n",
      "[Rank 0] Epoch 4 | Batch 60 | Loss: 0.4479\n",
      "[Rank 1] Epoch 4 | Batch 60 | Loss: 0.3579\n",
      "[Rank 2] Epoch 4 | Batch 60 | Loss: 0.6815\n",
      "[Rank 3] Epoch 4 | Batch 60 | Loss: 0.4247\n",
      "[Rank 3] Epoch 4 | Batch 70 | Loss: 0.3037\n",
      "[Rank 1] Epoch 4 | Batch 70 | Loss: 0.4284\n",
      "[Rank 2] Epoch 4 | Batch 70 | Loss: 0.3417\n",
      "[Rank 0] Epoch 4 | Batch 70 | Loss: 0.3564\n",
      "[Rank 1] Epoch 4 | Batch 80 | Loss: 0.4976\n",
      "[Rank 3] Epoch 4 | Batch 80 | Loss: 0.4792\n",
      "[Rank 2] Epoch 4 | Batch 80 | Loss: 0.3915\n",
      "[Rank 0] Epoch 4 | Batch 80 | Loss: 0.4654\n",
      "[Rank 3] Epoch 4 | Batch 90 | Loss: 0.5508\n",
      "[Rank 2] Epoch 4 | Batch 90 | Loss: 0.4794\n",
      "[Rank 1] Epoch 4 | Batch 90 | Loss: 0.4273\n",
      "[Rank 0] Epoch 4 | Batch 90 | Loss: 0.3871\n",
      "[Rank 1] Epoch 4 | Batch 100 | Loss: 0.3036\n",
      "[Rank 2] Epoch 4 | Batch 100 | Loss: 0.3909\n",
      "[Rank 0] Epoch 4 | Batch 100 | Loss: 0.3633\n",
      "[Rank 3] Epoch 4 | Batch 100 | Loss: 0.3984\n",
      "[Rank 1] Epoch 4 | Batch 110 | Loss: 0.3845\n",
      "[Rank 0] Epoch 4 | Batch 110 | Loss: 0.2260\n",
      "[Rank 3] Epoch 4 | Batch 110 | Loss: 0.3377\n",
      "[Rank 2] Epoch 4 | Batch 110 | Loss: 0.3275\n",
      "[Rank 2] Epoch 4 | Batch 120 | Loss: 0.4615\n",
      "[Rank 0] Epoch 4 | Batch 120 | Loss: 0.4148\n",
      "[Rank 3] Epoch 4 | Batch 120 | Loss: 0.3161\n",
      "[Rank 1] Epoch 4 | Batch 120 | Loss: 0.3845\n",
      "[Rank 0] Epoch 4 | Batch 130 | Loss: 0.3842\n",
      "[Rank 2] Epoch 4 | Batch 130 | Loss: 0.3029\n",
      "[Rank 3] Epoch 4 | Batch 130 | Loss: 0.4709\n",
      "[Rank 1] Epoch 4 | Batch 130 | Loss: 0.3244\n",
      "[Rank 0] Epoch 4 | Batch 140 | Loss: 0.5055\n",
      "[Rank 2] Epoch 4 | Batch 140 | Loss: 0.3339\n",
      "[Rank 3] Epoch 4 | Batch 140 | Loss: 0.6956\n",
      "[Rank 1] Epoch 4 | Batch 140 | Loss: 0.4442\n",
      "[Rank 2] Epoch 4 | Batch 150 | Loss: 0.5208\n",
      "[Rank 0] Epoch 4 | Batch 150 | Loss: 0.6393\n",
      "[Rank 3] Epoch 4 | Batch 150 | Loss: 0.4188\n",
      "[Rank 1] Epoch 4 | Batch 150 | Loss: 0.4287\n",
      "[Rank 1] Epoch 4 | Batch 160 | Loss: 0.3061\n",
      "[Rank 2] Epoch 4 | Batch 160 | Loss: 0.3278\n",
      "[Rank 3] Epoch 4 | Batch 160 | Loss: 0.5632\n",
      "[Rank 0] Epoch 4 | Batch 160 | Loss: 0.5387\n",
      "[Rank 2] Epoch 4 | Batch 170 | Loss: 0.3285\n",
      "[Rank 1] Epoch 4 | Batch 170 | Loss: 0.4771\n",
      "[Rank 0] Epoch 4 | Batch 170 | Loss: 0.4428\n",
      "[Rank 3] Epoch 4 | Batch 170 | Loss: 0.4045\n",
      "[Rank 3] Epoch 4 | Batch 180 | Loss: 0.2996\n",
      "[Rank 1] Epoch 4 | Batch 180 | Loss: 0.3760\n",
      "[Rank 2] Epoch 4 | Batch 180 | Loss: 0.3615\n",
      "[Rank 0] Epoch 4 | Batch 180 | Loss: 0.4137\n",
      "[Rank 2] Epoch 4 | Batch 190 | Loss: 0.4451\n",
      "[Rank 0] Epoch 4 | Batch 190 | Loss: 0.4317\n",
      "[Rank 1] Epoch 4 | Batch 190 | Loss: 0.6185\n",
      "[Rank 3] Epoch 4 | Batch 190 | Loss: 0.3390\n",
      "[Rank 2] Epoch 4 | Batch 200 | Loss: 0.3395\n",
      "[Rank 0] Epoch 4 | Batch 200 | Loss: 0.4120\n",
      "[Rank 3] Epoch 4 | Batch 200 | Loss: 0.4728\n",
      "[Rank 1] Epoch 4 | Batch 200 | Loss: 0.4310\n",
      "⏱️ [Rank 0] Epoch 4 completed in 470.39 seconds\n",
      "⏳ [Rank 0] Epoch 4 | Training Loss: 0.4246 | Accuracy: 80.03%\n",
      "⏱️ [Rank 2] Epoch 4 completed in 470.41 seconds\n",
      "⏳ [Rank 2] Epoch 4 | Training Loss: 0.4212 | Accuracy: 80.56%\n",
      "⏱️ [Rank 1] Epoch 4 completed in 470.35 seconds\n",
      "⏳ [Rank 1] Epoch 4 | Training Loss: 0.4244 | Accuracy: 80.33%\n",
      "⏱️ [Rank 3] Epoch 4 completed in 470.49 seconds\n",
      "⏳ [Rank 3] Epoch 4 | Training Loss: 0.4329 | Accuracy: 80.00%\n",
      "[Rank 2] Epoch 5 | Batch 0 | Loss: 0.3598\n",
      "[Rank 3] Epoch 5 | Batch 0 | Loss: 0.3389\n",
      "[Rank 1] Epoch 5 | Batch 0 | Loss: 0.4843\n",
      "[Rank 0] Epoch 5 | Batch 0 | Loss: 0.5292\n",
      "[Rank 0] Epoch 5 | Batch 10 | Loss: 0.2651\n",
      "[Rank 3] Epoch 5 | Batch 10 | Loss: 0.4536\n",
      "[Rank 2] Epoch 5 | Batch 10 | Loss: 0.3992\n",
      "[Rank 1] Epoch 5 | Batch 10 | Loss: 0.4750\n",
      "[Rank 2] Epoch 5 | Batch 20 | Loss: 0.4257\n",
      "[Rank 1] Epoch 5 | Batch 20 | Loss: 0.4014\n",
      "[Rank 0] Epoch 5 | Batch 20 | Loss: 0.3638\n",
      "[Rank 3] Epoch 5 | Batch 20 | Loss: 0.3956\n",
      "[Rank 0] Epoch 5 | Batch 30 | Loss: 0.3801\n",
      "[Rank 2] Epoch 5 | Batch 30 | Loss: 0.2360\n",
      "[Rank 1] Epoch 5 | Batch 30 | Loss: 0.3870\n",
      "[Rank 3] Epoch 5 | Batch 30 | Loss: 0.3477\n",
      "[Rank 3] Epoch 5 | Batch 40 | Loss: 0.4196\n",
      "[Rank 1] Epoch 5 | Batch 40 | Loss: 0.3934\n",
      "[Rank 2] Epoch 5 | Batch 40 | Loss: 0.2989\n",
      "[Rank 0] Epoch 5 | Batch 40 | Loss: 0.3736\n",
      "[Rank 0] Epoch 5 | Batch 50 | Loss: 0.3808\n",
      "[Rank 1] Epoch 5 | Batch 50 | Loss: 0.5179\n",
      "[Rank 2] Epoch 5 | Batch 50 | Loss: 0.3356\n",
      "[Rank 3] Epoch 5 | Batch 50 | Loss: 0.4032\n",
      "[Rank 0] Epoch 5 | Batch 60 | Loss: 0.3271\n",
      "[Rank 3] Epoch 5 | Batch 60 | Loss: 0.3959\n",
      "[Rank 2] Epoch 5 | Batch 60 | Loss: 0.3628\n",
      "[Rank 1] Epoch 5 | Batch 60 | Loss: 0.2400\n",
      "[Rank 2] Epoch 5 | Batch 70 | Loss: 0.3469\n",
      "[Rank 3] Epoch 5 | Batch 70 | Loss: 0.4949\n",
      "[Rank 0] Epoch 5 | Batch 70 | Loss: 0.5099\n",
      "[Rank 1] Epoch 5 | Batch 70 | Loss: 0.5205\n",
      "[Rank 0] Epoch 5 | Batch 80 | Loss: 0.4605\n",
      "[Rank 2] Epoch 5 | Batch 80 | Loss: 0.4394\n",
      "[Rank 3] Epoch 5 | Batch 80 | Loss: 0.4772\n",
      "[Rank 1] Epoch 5 | Batch 80 | Loss: 0.3590\n",
      "[Rank 1] Epoch 5 | Batch 90 | Loss: 0.4287\n",
      "[Rank 2] Epoch 5 | Batch 90 | Loss: 0.3498\n",
      "[Rank 3] Epoch 5 | Batch 90 | Loss: 0.5599\n",
      "[Rank 0] Epoch 5 | Batch 90 | Loss: 0.6959\n",
      "[Rank 0] Epoch 5 | Batch 100 | Loss: 0.2803\n",
      "[Rank 2] Epoch 5 | Batch 100 | Loss: 0.3497\n",
      "[Rank 3] Epoch 5 | Batch 100 | Loss: 0.4022\n",
      "[Rank 1] Epoch 5 | Batch 100 | Loss: 0.4574\n",
      "[Rank 0] Epoch 5 | Batch 110 | Loss: 0.3651\n",
      "[Rank 3] Epoch 5 | Batch 110 | Loss: 0.3211\n",
      "[Rank 2] Epoch 5 | Batch 110 | Loss: 0.4174\n",
      "[Rank 1] Epoch 5 | Batch 110 | Loss: 0.3781\n",
      "[Rank 0] Epoch 5 | Batch 120 | Loss: 0.2506\n",
      "[Rank 3] Epoch 5 | Batch 120 | Loss: 0.4516\n",
      "[Rank 1] Epoch 5 | Batch 120 | Loss: 0.4814\n",
      "[Rank 2] Epoch 5 | Batch 120 | Loss: 0.4023\n",
      "[Rank 2] Epoch 5 | Batch 130 | Loss: 0.3938\n",
      "[Rank 1] Epoch 5 | Batch 130 | Loss: 0.4630\n",
      "[Rank 3] Epoch 5 | Batch 130 | Loss: 0.4795\n",
      "[Rank 0] Epoch 5 | Batch 130 | Loss: 0.4242\n",
      "[Rank 3] Epoch 5 | Batch 140 | Loss: 0.4784\n",
      "[Rank 2] Epoch 5 | Batch 140 | Loss: 0.5519\n",
      "[Rank 0] Epoch 5 | Batch 140 | Loss: 0.4014\n",
      "[Rank 1] Epoch 5 | Batch 140 | Loss: 0.3157\n",
      "[Rank 3] Epoch 5 | Batch 150 | Loss: 0.3025\n",
      "[Rank 0] Epoch 5 | Batch 150 | Loss: 0.5862\n",
      "[Rank 1] Epoch 5 | Batch 150 | Loss: 0.4670\n",
      "[Rank 2] Epoch 5 | Batch 150 | Loss: 0.4779\n",
      "[Rank 2] Epoch 5 | Batch 160 | Loss: 0.4280\n",
      "[Rank 1] Epoch 5 | Batch 160 | Loss: 0.2364\n",
      "[Rank 0] Epoch 5 | Batch 160 | Loss: 0.3871\n",
      "[Rank 3] Epoch 5 | Batch 160 | Loss: 0.2629\n",
      "[Rank 2] Epoch 5 | Batch 170 | Loss: 0.4362\n",
      "[Rank 3] Epoch 5 | Batch 170 | Loss: 0.5006\n",
      "[Rank 0] Epoch 5 | Batch 170 | Loss: 0.3549\n",
      "[Rank 1] Epoch 5 | Batch 170 | Loss: 0.4200\n",
      "[Rank 3] Epoch 5 | Batch 180 | Loss: 0.3259\n",
      "[Rank 2] Epoch 5 | Batch 180 | Loss: 0.3309\n",
      "[Rank 1] Epoch 5 | Batch 180 | Loss: 0.5214\n",
      "[Rank 0] Epoch 5 | Batch 180 | Loss: 0.3682\n",
      "[Rank 1] Epoch 5 | Batch 190 | Loss: 0.3952\n",
      "[Rank 0] Epoch 5 | Batch 190 | Loss: 0.3693\n",
      "[Rank 3] Epoch 5 | Batch 190 | Loss: 0.4690\n",
      "[Rank 2] Epoch 5 | Batch 190 | Loss: 0.4393\n",
      "[Rank 1] Epoch 5 | Batch 200 | Loss: 0.6113\n",
      "[Rank 3] Epoch 5 | Batch 200 | Loss: 0.3096\n",
      "[Rank 2] Epoch 5 | Batch 200 | Loss: 0.3726\n",
      "[Rank 0] Epoch 5 | Batch 200 | Loss: 0.5317\n",
      "⏱️ [Rank 1] Epoch 5 completed in 468.74 seconds\n",
      "⏳ [Rank 1] Epoch 5 | Training Loss: 0.4213 | Accuracy: 80.57%\n",
      "⏱️ [Rank 3] Epoch 5 completed in 468.73 seconds\n",
      "⏳ [Rank 3] Epoch 5 | Training Loss: 0.4180 | Accuracy: 80.81%\n",
      "⏱️ [Rank 2] Epoch 5 completed in 468.85 seconds\n",
      "⏳ [Rank 2] Epoch 5 | Training Loss: 0.4153 | Accuracy: 81.07%\n",
      "⏱️ [Rank 0] Epoch 5 completed in 468.86 seconds\n",
      "⏳ [Rank 0] Epoch 5 | Training Loss: 0.4247 | Accuracy: 80.36%\n",
      "✅ Model saved as cnn_ddp_model.pth\n",
      "Figure(1000x500)\n"
     ]
    }
   ],
   "source": [
    "!python ddp_train_cpu.py \\\n",
    "  --image-dir \"/scratch/mohammed.moi/yolo_workspace/pneumonia_dataset/Pneumonia Dataset/Training/Images\" \\\n",
    "  --mask-dir \"/scratch/mohammed.moi/yolo_workspace/pneumonia_dataset/Pneumonia Dataset/Training/Masks\" \\\n",
    "  --save-dir \"/scratch/mohammed.moi/yolo_workspace\" \\\n",
    "  --world-size 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1ab348-3826-4cbe-ac3b-77471129595b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e852a2-25bc-414f-aaca-c7bca9f184a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e9e799-7c65-439d-81d8-065655ffa0e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e67bbfa-1f2e-43a0-bc1b-291fd0f7a001",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9519ea-40a4-41f3-9aa7-74a8eea2fbe1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059d2a06-629a-4796-8ce0-11282316fbb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94172ad-cc72-4bbe-ac15-ae23874ac1a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602cfea0-6af8-4857-afec-00d05e64270d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f95934-c3d0-4ff6-9b3f-27e198d012ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f10eea-1460-4a92-a68b-c4e6c279c479",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
